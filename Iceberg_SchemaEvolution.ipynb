{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdc48d35-9b9c-4596-b87c-189edce771fd",
   "metadata": {},
   "source": [
    "##### ICEBERG - SCHEMA EVOLUTION\n",
    "##### Objective: To test the schema evolution by adding, updating, deleting columns, and changing the datatype of columns \n",
    "##### Dataset format: This nested JSON dataset is about meteorite landing which is downloaded from the public source mentioned below\n",
    "##### Dataset Source: https://catalog.data.gov/dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d217b6da-bf28-4a79-971e-958c53aa0230",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"Drop Table icecatalog.mydb.meteorites\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297208f6-3d35-4615-b8e3-4ffd099e4ad0",
   "metadata": {},
   "source": [
    "##### 1.1 Configure Spark Iceberg Runtime package and other settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca73849d-b938-4f22-8048-f8c3502648d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark-sql.packages\", \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.2\")\n",
    "spark.conf.set(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n",
    "spark.conf.set(\"spark.sql.catalog.mycatalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\")\n",
    "spark.conf.set(\"spark.sql.catalog.mycatalog.type\", \"Hadoop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6928c58d-e221-4d2b-849a-e95442f81720",
   "metadata": {},
   "source": [
    "##### packages that need to be downloaded and used during the Spark session.\n",
    "###### spark.conf.set(\"spark-sql.packages\", \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.2\")\n",
    "\n",
    "##### This specifies any extensions to SQL that should be present in the Spark session\n",
    "###### spark.conf.set(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n",
    "\n",
    "##### The settings below are to configure your specific catalog, which can be under a namespace of your choosing eg: here it is spark_catalog\n",
    "##### This specifies that this specific catalog is using the Apache Iceberg Spark Catalog class.\n",
    "###### spark.conf.set(\"spark.sql.catalog.mycatalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\")\n",
    "\n",
    "##### This setting is used to set the type of catalog you are using, and possible values include:Hadoop (if using HDFS/File System Catalog), Hive\n",
    "###### spark.conf.set(\"spark.sql.catalog.mycatalog.type\", \"Hadoop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e43286-a5f9-455c-af9d-32c542f47916",
   "metadata": {},
   "source": [
    "##### 1.2 The spark.sql.extensions might show an error that \"Cannot modify the value of a static config: spark.sql.extensions\" which means it's not set, However, when its value is checked Using getter configuration, it displays the value as set above i.e. IcebergSparkSessionExtensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010b0861-bfca-43ed-9e61-ce2738f59e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.get(\"spark.sql.extensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885bc0ab-14f6-4668-8f08-f9b6b4171c6f",
   "metadata": {},
   "source": [
    "##### 2.0 Read the nested json dataset and verify its schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d14f5f-ae07-46db-91e4-7341e37ca3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "val jsonDF =  spark.read.option(\"multiline\", \"true\").json(\"./Data/Json/meteorite_landing.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9320a79-8a8d-4fdb-9255-b34332915659",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonDF.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a2d49c-7aea-4122-9119-e58e6290fc82",
   "metadata": {},
   "source": [
    "##### source: https://stackoverflow.com/questions/61863489/flatten-nested-json-in-scala-spark-dataframe/61863579#61863579\n",
    "##### 2.1 The dynamic code in scala is referred from the above source where it explodes the nested JSON into individual columns, since all of these are not arrays we cannot use the explode function and also it will make it more cumbersome to process individual columns. The below code dynamically splits arrays, and structs type into individual columns keeping its hierarchy intact, For eg if c is the nested child of b which is a child of an i.e. a.b.c, the code will split it as an a_b_c column. This will also prevent duplicating columns in case nested JSON has the same property name because its specific hierarchy will be attached to its name now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24830e28-8d1b-4e0e-bdcf-a25011b69b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "import scala.annotation.tailrec\n",
    "import scala.util.Try\n",
    "\n",
    "implicit class DFHelpers(df: DataFrame) {\n",
    "    def columns = {\n",
    "      val dfColumns = df.columns.map(_.toLowerCase)\n",
    "      df.schema.fields.flatMap { data =>\n",
    "        data match {\n",
    "          case column if column.dataType.isInstanceOf[StructType] => {\n",
    "            column.dataType.asInstanceOf[StructType].fields.map { field =>\n",
    "              val columnName = column.name\n",
    "              val fieldName = field.name\n",
    "              col(s\"${columnName}.${fieldName}\").as(s\"${columnName}_${fieldName}\")\n",
    "            }.toList\n",
    "          }\n",
    "          case column => List(col(s\"${column.name}\"))\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "\n",
    "    def flatten: DataFrame = {\n",
    "      val empty = df.schema.filter(_.dataType.isInstanceOf[StructType]).isEmpty\n",
    "      empty match {\n",
    "        case false =>\n",
    "          df.select(columns: _*).flatten\n",
    "        case _ => df\n",
    "      }\n",
    "    }\n",
    "    def explodeColumns = {\n",
    "      @tailrec\n",
    "      def columns(cdf: DataFrame):DataFrame = cdf.schema.fields.filter(_.dataType.typeName == \"array\") match {\n",
    "        case c if !c.isEmpty => columns(c.foldLeft(cdf)((dfa,field) => {\n",
    "          dfa.withColumn(field.name,explode_outer(col(s\"${field.name}\"))).flatten\n",
    "        }))\n",
    "        case _ => cdf\n",
    "      }\n",
    "      columns(df.flatten)\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651e505c-3c7e-481d-b11d-138186ca18c3",
   "metadata": {},
   "source": [
    "##### 2.2 Call the above function with the dataframe variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8197fc02-4c7e-49d7-8fc1-da458dabf717",
   "metadata": {},
   "outputs": [],
   "source": [
    "val flattenedJsonDF = jsonDF.explodeColumns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b2c2fd-1278-4efd-8fd9-e2817ea5541f",
   "metadata": {},
   "source": [
    "##### 2.3 Verifying the schema after the function is run. It maintained the hierarchy by separating the names with underscore symbole ('-') between each data column. This will also prevent duplicating columns in case nested json has the same property name because its specific hierarchy will be attached to its name now.\n",
    "##### For eg: the property 'flags' are now prefixed with its hierarchy meta_view_flags, meta_view_owner_flags, meta_view_columns_flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d2868e-2488-4481-bc62-9c7e1afe63c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "flattenedJsonDF.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b390ea0e-9172-4057-a2bc-6ea1a1541e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "flattenedJsonDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19393a03-ff88-46e5-8448-02ce5215fe66",
   "metadata": {},
   "source": [
    "##### 3. Finally :-), the core step, create an iceberg table and write the json data to it. \n",
    "##### Once the catalog is created, for the next writes use append to add to an existing table, create for new , replace to overwrite, or use both createOrReplace for the safer side if one wants to replace and create a new table if it already exists to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e6a8c8-b884-4883-9a8f-e77cff6d05b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "val icedTable = flattenedJsonDF.writeTo(\"icecatalog.mydb.meteorites\").using(\"iceberg\").createOrReplace()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6235ca2-84f2-42c7-8162-bb857de75b4f",
   "metadata": {},
   "source": [
    "##### 3.1 Verify if the table is created using iceberg format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2aa38b0-21ba-418c-8a4d-75cfd4f00c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "val meteorTable = spark.read.format(\"iceberg\").load(\"icecatalog.mydb.meteorites\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504b3548-e5e2-4b58-9438-48f78ade5a40",
   "metadata": {},
   "source": [
    "##### 3.2 Display result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b270c4bd-c74f-466a-91a8-f2f9ea5e6571",
   "metadata": {},
   "outputs": [],
   "source": [
    "meteorTable.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9cd832-e09e-44fb-9f5e-3103016fc194",
   "metadata": {},
   "source": [
    "##### 3.3 Check the count to test later when we append the data again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60af0ae-efef-42ce-a4f0-97c41b7c5fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "meteorTable.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a1e490-b09d-4fbf-bbdd-db475afc329c",
   "metadata": {},
   "source": [
    "##### 3.4 Check the count of columns to test the columsn size post delete step executions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdf046e-5cd2-44c9-8747-fb33194d278f",
   "metadata": {},
   "outputs": [],
   "source": [
    "meteorTable.columns.length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7f7ac7-7972-4710-bd34-03db23904f43",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### 4 The Schema Evolution Feature test Begins here. Not really :-p, It is a test before the actual test\n",
    "##### Testing if any of the cases of Schema Evolution works by default without further configurations. \n",
    "##### These 4.x steps are to verify if any of the schema evolution cases works by default before configuring schema evolution-specific settings. These executions are run to compare results before and after schema evolution configurations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40423bc3-c0dd-42eb-909a-9112f6f8d3b7",
   "metadata": {},
   "source": [
    "##### 4.1 From the original dataset, I have deleted and updated(which is as good as adding new column) columns and changed datatype of other column and saved them as new datasets, so that I can write back to the iceberg table where the schema is already set due to the first write execution\n",
    "##### New columns : Added a new string column meta_view_approvals_documentVersion, boolean column documentReviewed \n",
    "##### Updated(equivalent to new) columns: integer col from meta_view_averageRating to  meta_view_averageDocumentRating\n",
    "##### Deleted column : meta_view_hideFromCatalog\n",
    "##### Steps are - Read the modified JSON dataset, explode it, and then append it to the existing iceberg table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9430053a-c4bc-46d9-aeb1-bbf3e2c33bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "val allchangesColjsonDF =  spark.read.option(\"multiline\", \"true\").json(\"./Data/Json/meteorite_landing_change.json\")\n",
    "val flattenedallchangesDF = allchangesColjsonDF.explodeColumns\n",
    "val icedallchangedTable = flattenedallchangesDF.writeTo(\"icecatalog.mydb.meteorites\").append()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29988dc7-228c-4ee0-922a-85ea7498e019",
   "metadata": {},
   "source": [
    "##### 4.2.1 From the original dataset, I have deleted one column 'meta_view_hideFromCatalog' and saved as new dataset,a boolean column 'meta_view_hideFromCatalog' with value false and newBackend with value true and an integer column publicationGroup whose value is greater than zero. So that I can write back to iceberg table where schema is already set due to first write execution\n",
    "##### Steps are - Read the json dataset with one missing column, explode it and then append it to existing iceberg table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97495ed3-bf52-46ac-8bf2-97f9d92b7260",
   "metadata": {},
   "outputs": [],
   "source": [
    "val deletedColjsonDF =  spark.read.option(\"multiline\", \"true\").json(\"./Data/Json/meteorite_landing_deletedmulticols.json\")\n",
    "val flatteneddeletedColJsonDF = deletedColjsonDF.explodeColumns\n",
    "val icedDeletedColTable = flatteneddeletedColJsonDF.writeTo(\"icecatalog.mydb.meteorites\").append()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c991ec3c-702a-48d8-a085-7af2b49b4938",
   "metadata": {},
   "source": [
    "##### 4.3 From the original dataset, I have updated one column (which is as good as a new column now) and saved it as a new dataset. So that I can write back to the iceberg table where schema is already set due to the first write execution\n",
    "##### Updated column from meta_view_averageRating to meta_view_averageDocumentRating\n",
    "##### Steps - Read the json dataset with one new column, explode it, and then append it to the existing iceberg table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68978821-9c4c-4ea9-aa30-6ce9cb1b47f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "val updatedColjsonDF =  spark.read.option(\"multiline\", \"true\").json(\"./Data/Json/meteorite_landing_UpdatedCol.json\")\n",
    "val flattenedupdatedColJsonDF = updatedColjsonDF.explodeColumns\n",
    "val icedDeletedColTable = flattenedupdatedColJsonDF.writeTo(\"icecatalog.mydb.meteorites\").append()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231b834b-20eb-4886-ad13-0dee1b13edd8",
   "metadata": {},
   "source": [
    "##### 4.4 From the original dataset, I have changed the data type of column and saved it as a new dataset, So that I can write back to the iceberg table where the schema is already set due to the first write execution.\n",
    "##### changed the data type of column meta_view_createdAt from string to bigint(basically removed the double quotes around the value from JSON data)\n",
    "##### Read the JSON dataset with the changed data type column, explode it, and then append it to the existing iceberg table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f06306b-2ce0-49cf-8a8e-b1787dcc5bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "val deletedColjsonDF =  spark.read.option(\"multiline\", \"true\").json(\"./Data/Json/meteorite_landing_changeDT.json\")\n",
    "val flatteneddeletedColJsonDF = deletedColjsonDF.explodeColumns\n",
    "val icedDeletedColTable = flatteneddeletedColJsonDF.writeTo(\"icecatalog.mydb.meteorites\").append()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c89873-a69a-4484-96e6-7c9e7b4893c7",
   "metadata": {},
   "source": [
    "##### 5.0 SCHEMA EVOLUTION TEST(actually) BEGINS HERE \n",
    "##### Original Source: https://iceberg.apache.org/docs/latest/spark-writes/#schema-merge\n",
    "##### Set the table property to accept any schema as per the above Apache documentation source before \n",
    "\n",
    "##### The documentation from the above link states below\n",
    "##### Schema MergeðŸ”—\n",
    "##### While inserting or updating Iceberg is capable of resolving schema mismatch at runtime. \n",
    "##### If configured, Iceberg will perform an automatic schema evolution as follows:\n",
    "##### A new column is present in the source but not in the target table.\n",
    "##### The new column is added to the target table. Column values are set to NULL in all the rows already present in the table\n",
    "##### A column is present in the target but not in the source.\n",
    "##### The target column value is set to NULL when inserting or left unchanged when updating the row.\n",
    "##### The target table must be configured to accept any schema change by setting the property write.spark.accept-any-schema to true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e5603e-68d3-42ce-ab57-b4c48cbd0b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"ALTER TABLE icecatalog.mydb.meteorites SET TBLPROPERTIES ('write.spark.accept-any-schema'='true')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a262d433-5c7e-4b62-841c-0f7cd8bce4e6",
   "metadata": {},
   "source": [
    "##### 5.1 This step is the same as 4.1, I am using the dataset where all changes (add, delete, data type change) exist so that I can write back to the iceberg table where the schema is already set due to the first write execution\n",
    "##### New columns : Added a new string column meta_view_approvals_documentVersion, boolean column documentReviewed and updated(equivalent to new) integer col from meta_view_averageRating to  meta_view_averageDocumentRating\n",
    "##### Deleted column: meta_view_hideFromCatalog\n",
    "##### Read the JSON dataset with the changed data type column, explode it, and then append it to the existing iceberg table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1c4131-16d5-4baa-8db0-cd5c4a8f6bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "val allschemchangesDF =  spark.read.option(\"multiline\", \"true\").json(\"./Data/Json/meteorite_landing_change.json\")\n",
    "val allschemchangesFlatDF = allschemchangesDF.explodeColumns\n",
    "val allschemchangesIcedTable = allschemchangesFlatDF.writeTo(\"icecatalog.mydb.meteorites\").option(\"mergeSchema\",\"true\").append()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1ba861-3e5c-43eb-b707-7472ea585ac7",
   "metadata": {},
   "source": [
    "##### 5.2.1 This step is similar to 4.2\n",
    "##### From the original dataset, I have deleted three columns of different datatypes and saved them as a new dataset\n",
    "##### a boolean column 'meta_view_hideFromCatalog' with value false and newBackend with value true and an integer column publicationGroup whose value is greater than 0. So that I can write back to iceberg table where schema is already set due to the first write execution\n",
    "##### Read the json dataset with one missing column, explode it, and then append it to the existing iceberg table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e620c22e-ca59-4a8f-975b-6896b4d49857",
   "metadata": {},
   "outputs": [],
   "source": [
    "val allschemchangesDelDF =  spark.read.option(\"multiline\", \"true\").json(\"./Data/Json/meteorite_landing_deletedmulticols.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ded07b-f562-4e8f-ae57-3b6ee8448703",
   "metadata": {},
   "outputs": [],
   "source": [
    "val allschemchangesDelFlatDF = allschemchangesDelDF.explodeColumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e79f51-f127-4952-a0fb-154b5a55abd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "val allschemchangesDelIcedTable = allschemchangesDelFlatDF.writeTo(\"icecatalog.mydb.meteorites\").option(\"mergeSchema\",\"true\").append()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025a1abf-9e4c-47f2-a225-b9885f79340f",
   "metadata": {},
   "source": [
    "##### 5.2.2 Since the deletion works or at least appended without any errors, it's worth checking how the table schema looks now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5759ce-7aeb-4370-ba7a-96aabc945318",
   "metadata": {},
   "outputs": [],
   "source": [
    "val icedmeteorCheckTable = spark.read.format(\"iceberg\").load(\"icecatalog.mydb.meteorites\")\n",
    "icedmeteorCheckTable.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0e6c46-fe95-4f28-892a-5b78534ec378",
   "metadata": {},
   "source": [
    "##### 4.2.3 Verify the count as the data is successfully appended, it should be exactly twice the earlier result checked at 3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3ba3af-b4b7-472a-9d55-74f2b5be6dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "icedmeteorCheckTable.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f23b979-919a-4c56-98cd-78eb199a39ff",
   "metadata": {},
   "source": [
    "##### 4.2.4 Verify the columsn count as one column is deleted now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1c390d-736f-4b8e-8c13-ad86263dcc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "icedmeteorCheckTable.columns.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371c60ae-4f7f-4f62-b3df-580697d2f461",
   "metadata": {},
   "outputs": [],
   "source": [
    "icedmeteorCheckTable.select(\"meta_view_hideFromCatalog\",\"meta_view_newBackend\",\"meta_view_publicationGroup\").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc33fe6-5c7c-4aed-ab64-b011489e609c",
   "metadata": {},
   "outputs": [],
   "source": [
    "icedmeteorCheckTable.select(\"meta_view_hideFromCatalog\",\"meta_view_newBackend\",\"meta_view_publicationGroup\").tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dc3092-30a5-4195-b8d0-51115b423159",
   "metadata": {},
   "source": [
    "##### 5.3 This step is the same and using the same dataset as 4.3\n",
    "##### From the original dataset, I have updated one column (which is as good as a new column now) and saved as a new dataset, so that I can write back to the iceberg table where schema is already set due to the first write execution\n",
    "##### Updated column from meta_view_averageRating to meta_view_averageDocumentRating\n",
    "##### Steps - Read the JSON dataset with one new column, explode it, and then append it to the existing iceberg table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45db001a-65a3-4bc9-871a-80bfef5ae67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val allschemchangesUpDF =  spark.read.option(\"multiline\", \"true\").json(\"./Data/Json/meteorite_landing_UpdatedCol.json\")\n",
    "val allschemchangesUpFlatDF = allschemchangesUpDF.explodeColumns\n",
    "val allschemchangesUpIcedTable = allschemchangesUpFlatDF.writeTo(\"icecatalog.mydb.meteorites\").option(\"mergeSchema\",\"true\").append()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea58257-ea02-4dc1-ac6d-cc919f4a5ca1",
   "metadata": {},
   "source": [
    "##### 5.4.1 This step is the same as 4.4 \n",
    "##### From the original dataset, I have changed the data type of column  and saved as a new dataset, so that I can write back to the iceberg table where the schema is already set due to the first write execution\n",
    "##### changed the data type of column meta_view_createdAt from string to bigint(basically removed the double quotes around the value from JSON data)\n",
    "##### Read the json dataset with the changed data type column, explode it, and then append it to the existing iceberg table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f00582-0a18-473c-b2d5-e3335a903f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "val allschemchangesTypeDF =  spark.read.option(\"multiline\", \"true\").json(\"./Data/Json/meteorite_landing_changeDT.json\")\n",
    "val allschemchangesTypeFlatDF = allschemchangesTypeDF.explodeColumns\n",
    "val allschemchangesTypeIcedTable = allschemchangesTypeFlatDF.writeTo(\"icecatalog.mydb.meteorites\").option(\"mergeSchema\",\"true\").append()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83f5a90-2a8c-4e92-a851-fd78681c69e6",
   "metadata": {},
   "source": [
    "##### 5.4.2 This step is similar to 4.4 however here, I have downcasted the column datatype to check if downcasting works\n",
    "##### From the original dataset, I have changed the datat type of column  and saved as new dataset, so that I can write back to iceberg table where schema is already set due to first write execution\n",
    "##### changed the data type of column meta_view_averageRating from LONG to DOUBLE\n",
    "##### Read the json dataset with changed data type column, explode it and then append it to existing iceberg table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74333aa-aa08-4540-ac21-b8d832280e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "val allschemchangesDownTypeDF =  spark.read.option(\"multiline\", \"true\").json(\"./Data/Json/meteorite_landing-changeDTup.json\")\n",
    "val allschemchangesDownTypeFlatDF = allschemchangesDownTypeDF.explodeColumns\n",
    "val allschemchangesDownTypeIcedTable = allschemchangesDownTypeFlatDF.writeTo(\"icecatalog.mydb.meteorites\").option(\"mergeSchema\",\"true\").append()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
