{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a77c7ec4-d201-43b0-b782-7860b5724790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://bc18817f1332:4043\n",
       "SparkContext available as 'sc' (version = 3.5.1, master = local[*], app id = local-1720749578446)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "deleteStrDataDF: org.apache.spark.sql.DataFrame = [data: array<array<string>>, meta: struct<view: struct<approvals: array<struct<reviewedAt:bigint,reviewedAutomatically:boolean,state:string,submissionDetails:struct<permissionType:string>,submissionId:bigint,submissionObject:string,submissionOutcome:string,submissionOutcomeApplication:struct<failureCount:bigint,status:string>,submittedAt:bigint,submitter:struct<displayName:string,id:string>,targetAudience:string,workflowId:bigint>>, assetType: string ... 37 more fields>>]\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val deleteStrDataDF =  spark.read.option(\"multiline\", \"true\").json(\"./Data/Json/meteorite_landing-delStrCategory.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214d3c04-5f0e-445b-814d-3fe179bdf0be",
   "metadata": {},
   "source": [
    "##### source: https://stackoverflow.com/questions/61863489/flatten-nested-json-in-scala-spark-dataframe/61863579#61863579\n",
    "##### The below dynamic code in scala is referred from the above source where it explodes the nested JSON into individual columns, since all of these are not arrays we cannot use the explode function and also it will make it more cumbersome to process individual columns. The below code dynamically splits arrays, and structs type into individual columns keeping its hierarchy intact, For eg if c is the nested child of b which is a child of an i.e. a.b.c, the code will split it as an a_b_c column. This will also prevent duplicating columns in case nested JSON has the same property name because its specific hierarchy will be attached to its name now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49a8f97a-097d-4b6e-b692-505c79f0b9a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.sql.types._\n",
       "import scala.annotation.tailrec\n",
       "import scala.util.Try\n",
       "defined class DFHelpers\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "import scala.annotation.tailrec\n",
    "import scala.util.Try\n",
    "\n",
    "implicit class DFHelpers(df: DataFrame) {\n",
    "    def columns = {\n",
    "      val dfColumns = df.columns.map(_.toLowerCase)\n",
    "      df.schema.fields.flatMap { data =>\n",
    "        data match {\n",
    "          case column if column.dataType.isInstanceOf[StructType] => {\n",
    "            column.dataType.asInstanceOf[StructType].fields.map { field =>\n",
    "              val columnName = column.name\n",
    "              val fieldName = field.name\n",
    "              col(s\"${columnName}.${fieldName}\").as(s\"${columnName}_${fieldName}\")\n",
    "            }.toList\n",
    "          }\n",
    "          case column => List(col(s\"${column.name}\"))\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "\n",
    "    def flatten: DataFrame = {\n",
    "      val empty = df.schema.filter(_.dataType.isInstanceOf[StructType]).isEmpty\n",
    "      empty match {\n",
    "        case false =>\n",
    "          df.select(columns: _*).flatten\n",
    "        case _ => df\n",
    "      }\n",
    "    }\n",
    "    def explodeColumns = {\n",
    "      @tailrec\n",
    "      def columns(cdf: DataFrame):DataFrame = cdf.schema.fields.filter(_.dataType.typeName == \"array\") match {\n",
    "        case c if !c.isEmpty => columns(c.foldLeft(cdf)((dfa,field) => {\n",
    "          dfa.withColumn(field.name,explode_outer(col(s\"${field.name}\"))).flatten\n",
    "        }))\n",
    "        case _ => cdf\n",
    "      }\n",
    "      columns(df.flatten)\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31374c48-aecc-4733-9574-effbedff11d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "exdeleteStrDataDF: org.apache.spark.sql.DataFrame = [data: string, meta_view_approvals_reviewedAt: bigint ... 99 more fields]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val exdeleteStrDataDF = deleteStrDataDF.explodeColumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59f77f33-088b-415d-af9c-8c63136ddd34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: org.apache.spark.sql.DataFrame = []\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"ALTER TABLE icedsparkcatalog.mydb.meteorites SET TBLPROPERTIES ('write.spark.accept-any-schema'='true')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e2bb489-f9d6-4a24-88eb-d702f4cf1c52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "delstrIcedAppendTable: Unit = ()\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val delstrIcedAppendTable = exdeleteStrDataDF.writeTo(\"icedsparkcatalog.mydb.meteorites\").option(\"mergeSchema\",\"true\").append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "387bcc1b-557e-4bcc-a4da-4759ebec7e06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "delstrIcedTable: org.apache.spark.sql.DataFrame = [data: string, meta_view_approvals_reviewedAt: bigint ... 100 more fields]\n",
       "res1: Int = 102\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val delstrIcedTable = spark.read.format(\"iceberg\").load(\"icedsparkcatalog.mydb.meteorites\")\n",
    "delstrIcedTable.columns.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d19e31db-0e9a-4bc8-b9ff-e3053a980c1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res8: Array[org.apache.spark.sql.Row] = Array([null], [null], [null], [null], [null], [null], [null], [null], [null], [null])\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delstrIcedTable.select(\"meta_view_category\").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bdccbc2-2030-4bc6-9c50-dfe18e116f4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res9: Array[org.apache.spark.sql.Row] = Array([Space Science], [Space Science], [Space Science], [Space Science], [Space Science], [Space Science], [Space Science], [Space Science], [Space Science], [Space Science])\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delstrIcedTable.select(\"meta_view_category\").tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "542c6062-9971-45e0-b32d-19ab865800de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|meta_view_category|\n",
      "+------------------+\n",
      "|              NULL|\n",
      "|              NULL|\n",
      "|              NULL|\n",
      "|              NULL|\n",
      "|              NULL|\n",
      "|              NULL|\n",
      "|              NULL|\n",
      "|              NULL|\n",
      "|              NULL|\n",
      "|              NULL|\n",
      "|              NULL|\n",
      "|              NULL|\n",
      "|              NULL|\n",
      "|              NULL|\n",
      "|              NULL|\n",
      "|              NULL|\n",
      "|              NULL|\n",
      "|              NULL|\n",
      "|              NULL|\n",
      "|              NULL|\n",
      "+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delstrIcedTable.select(\"meta_view_category\").filter(\"meta_view_category is null\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68bec5d3-bb64-41ea-a781-7b76c6730ab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res5: Long = 542500\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delstrIcedTable.select(\"meta_view_category\").filter(\"meta_view_category is null\").count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
