{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a77c7ec4-d201-43b0-b782-7860b5724790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deleteIntDataDF: org.apache.spark.sql.DataFrame = [data: array<array<string>>, meta: struct<view: struct<approvals: array<struct<reviewedAt:bigint,reviewedAutomatically:boolean,state:string,submissionDetails:struct<permissionType:string>,submissionId:bigint,submissionObject:string,submissionOutcome:string,submissionOutcomeApplication:struct<failureCount:bigint,status:string>,submittedAt:bigint,submitter:struct<displayName:string,id:string>,targetAudience:string,workflowId:bigint>>, assetType: string ... 37 more fields>>]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val deleteIntDataDF =  spark.read.option(\"multiline\", \"true\").json(\"./Data/Json/meteorite_landing-delAvgRat.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b452e808-7e22-48ce-8fe7-6c2dfcafad5b",
   "metadata": {},
   "source": [
    "##### source: https://stackoverflow.com/questions/61863489/flatten-nested-json-in-scala-spark-dataframe/61863579#61863579\n",
    "#####  The below dynamic code in scala is referred from the above source where it explodes the nested JSON into individual columns, since all of these are not arrays we cannot use the explode function and also it will make it more cumbersome to process individual columns. The below code dynamically splits arrays, and structs type into individual columns keeping its hierarchy intact, For eg if c is the nested child of b which is a child of an i.e. a.b.c, the code will split it as an a_b_c column. This will also prevent duplicating columns in case nested JSON has the same property name because its specific hierarchy will be attached to its name now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49a8f97a-097d-4b6e-b692-505c79f0b9a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.sql.types._\n",
       "import scala.annotation.tailrec\n",
       "import scala.util.Try\n",
       "defined class DFHelpers\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "import scala.annotation.tailrec\n",
    "import scala.util.Try\n",
    "\n",
    "implicit class DFHelpers(df: DataFrame) {\n",
    "    def columns = {\n",
    "      val dfColumns = df.columns.map(_.toLowerCase)\n",
    "      df.schema.fields.flatMap { data =>\n",
    "        data match {\n",
    "          case column if column.dataType.isInstanceOf[StructType] => {\n",
    "            column.dataType.asInstanceOf[StructType].fields.map { field =>\n",
    "              val columnName = column.name\n",
    "              val fieldName = field.name\n",
    "              col(s\"${columnName}.${fieldName}\").as(s\"${columnName}_${fieldName}\")\n",
    "            }.toList\n",
    "          }\n",
    "          case column => List(col(s\"${column.name}\"))\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "\n",
    "    def flatten: DataFrame = {\n",
    "      val empty = df.schema.filter(_.dataType.isInstanceOf[StructType]).isEmpty\n",
    "      empty match {\n",
    "        case false =>\n",
    "          df.select(columns: _*).flatten\n",
    "        case _ => df\n",
    "      }\n",
    "    }\n",
    "    def explodeColumns = {\n",
    "      @tailrec\n",
    "      def columns(cdf: DataFrame):DataFrame = cdf.schema.fields.filter(_.dataType.typeName == \"array\") match {\n",
    "        case c if !c.isEmpty => columns(c.foldLeft(cdf)((dfa,field) => {\n",
    "          dfa.withColumn(field.name,explode_outer(col(s\"${field.name}\"))).flatten\n",
    "        }))\n",
    "        case _ => cdf\n",
    "      }\n",
    "      columns(df.flatten)\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31374c48-aecc-4733-9574-effbedff11d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "exdeleteIntDataDF: org.apache.spark.sql.DataFrame = [data: string, meta_view_approvals_reviewedAt: bigint ... 99 more fields]\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val exdeleteIntDataDF = deleteIntDataDF.explodeColumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59f77f33-088b-415d-af9c-8c63136ddd34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: org.apache.spark.sql.DataFrame = []\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"ALTER TABLE icedsparkcatalog.mydb.meteorites SET TBLPROPERTIES ('write.spark.accept-any-schema'='true')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e2bb489-f9d6-4a24-88eb-d702f4cf1c52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deleteIntAppendIcedTable: Unit = ()\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val deleteIntAppendIcedTable = exdeleteIntDataDF.writeTo(\"icedsparkcatalog.mydb.meteorites\").option(\"mergeSchema\",\"true\").append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2e367d5-ebdd-4fc8-ac83-71d45550309b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deleteIntIcedTable: org.apache.spark.sql.DataFrame = [data: string, meta_view_approvals_reviewedAt: bigint ... 100 more fields]\n",
       "res2: Int = 102\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val deleteIntIcedTable = spark.read.format(\"iceberg\").load(\"icedsparkcatalog.mydb.meteorites\")\n",
    "deleteIntIcedTable.columns.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f424901e-2507-453e-b2f8-35374547843e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res13: Array[org.apache.spark.sql.Row] = Array([null], [null], [null], [null], [null], [null], [null], [null], [null], [null])\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deleteIntIcedTable.select(\"meta_view_AverageRating\").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f91a6344-602d-4ada-a05d-ae8306a39fe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res14: Array[org.apache.spark.sql.Row] = Array([0], [0], [0], [0], [0], [0], [0], [0], [0], [0])\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deleteIntIcedTable.select(\"meta_view_AverageRating\").tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5bb8b215-ed14-4d5d-bf3c-1fe5f42558eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res10: Long = 542500\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deleteIntIcedTable.select(\"meta_view_AverageRating\").filter(\"meta_view_AverageRating is null\").count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
